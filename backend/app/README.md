<!-- markdownlint-disable MD029 -->
# LearnIA - Backend API

API REST para chat com modelos de IA, desenvolvida em FastAPI com arquitetura modular e suporte a m√∫ltiplos provedores de IA.

## üöÄ Tecnologias Utilizadas

- **FastAPI** - Framework web moderno e r√°pido para Python
- **Google Gemini API** - Integra√ß√£o com modelos de IA do Google
- **Uvicorn** - Servidor ASGI para aplica√ß√µes Python
- **Python 3.14+** - Linguagem de programa√ß√£o
- **UV** - Gerenciador de pacotes e depend√™ncias Python

## üìÅ Estrutura do Projeto

```bash
backend/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ AIChat/                 # M√≥dulo principal de IA
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent.py           # Classe base abstrata para agentes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent_factory.py   # Factory para cria√ß√£o de agentes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent_models_enum.py # Enum com modelos dispon√≠veis
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gemini_agent.py    # Implementa√ß√£o do agente Gemini
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ message.py         # Model Pydantic para mensagens
‚îÇ   ‚îú‚îÄ‚îÄ dependencies.py        # Depend√™ncias do FastAPI
‚îÇ   ‚îú‚îÄ‚îÄ main.py               # Aplica√ß√£o principal FastAPI
‚îÇ   ‚îî‚îÄ‚îÄ settings.py           # Configura√ß√µes de vari√°veis de ambiente da aplica√ß√£o
‚îú‚îÄ‚îÄ Dockerfile                # Container Docker
‚îî‚îÄ‚îÄ pyproject.toml           # Configura√ß√£o do projeto Python
```

## üèóÔ∏è Arquitetura

O backend utiliza o padr√£o **Factory** para criar agentes de IA e o padr√£o **Strategy** para diferentes implementa√ß√µes de modelos:

### Componentes Principais

1. **Agent (Classe Abstrata)**: Define a interface comum para todos os agentes de IA
2. **AgentFactory**: Respons√°vel pela cria√ß√£o de inst√¢ncias de agentes espec√≠ficos
3. **GeminiAgent**: Implementa√ß√£o concreta para o modelo Google Gemini
4. **Message**: Model Pydantic para valida√ß√£o e serializa√ß√£o de mensagens
5. **AgentModelEnum**: Enum que define os modelos de IA dispon√≠veis

### Fluxo de Dados

```bash
Cliente ‚Üí FastAPI ‚Üí AgentFactory ‚Üí GeminiAgent ‚Üí Google Gemini API ‚Üí Resposta
```

## üîß Instala√ß√£o e Configura√ß√£o

### Pr√©-requisitos

- Python 3.14 ou superior
- UV (gerenciador de pacotes)

### Instala√ß√£o

1. **Navegue at√© o diret√≥rio do backend:**

```bash
cd backend
```

2. **Instale as depend√™ncias:**

```bash
uv sync
```

3. **Configure as vari√°veis de ambiente:**

```bash
# Crie um arquivo .env (opcional)
ALLOW_HOSTS=["http://localhost:3000","https://yourdomain.com"]
```

## üöÄ Execu√ß√£o

### Desenvolvimento

```bash
# Com UV
uv run uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

# Ou diretamente
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

### Produ√ß√£o

```bash
uv run uvicorn app.main:app --host 0.0.0.0 --port 8000
```

### Docker

```bash
# Build
docker build -t learnai-backend .

# Run
docker run -p 8000:8000 learnai-backend -e ALLOW_HOSTS=["http://localhost:3000"]
```

## üìã Endpoints da API

### `POST /ask`

Envia mensagens para o modelo de IA selecionado.
**Query Parameters:**

- `model` (required): Modelo de IA (ex: `gemini`)

**Body:**

```json

{
  "apiKey": "sua-api-key",
  "messages": [
    {
      "role": "user", 
      "content": "Sua pergunta aqui"
    }
  ]
}
```

**Response:**

```json
{
  "message": "Resposta do modelo de IA"
}
```

### `GET /agents`

Lista todos os modelos de IA dispon√≠veis.

**Response:**

```json
["gemini"]
```

### `GET /system-prompt`

Retorna o prompt de sistema padr√£o usado pela aplica√ß√£o.

**Response:**

```json
{
  "system_prompt": "## Prompt de Sistema ‚Äî Professor de Tecnologia\n..."
}
```

## üß™ Testes e Qualidade de C√≥digo

### Linting

```bash
# Verificar c√≥digo com Ruff
uvx ruff check .

# Corrigir problemas automaticamente
uvx ruff check . --fix
```

### Testes (Em desenvolvimento)

```bash
# Executar testes (quando implementados)
uv run pytest
```

## üîå Adicionando Novos Modelos de IA

Para adicionar suporte a um novo modelo:

1. **Adicione o modelo ao enum:**

```python
# agent_models_enum.py
class AgentModelEnum(Enum):
    GEMINI = "gemini"
    NOVO_MODEL = "novo_model"  # Adicione aqui
```

2. **Crie a implementa√ß√£o do agente:**

```python
# novo_model_agent.py
from app.AIChat.agent import Agent

class NovoModelAgent(Agent):
    def chat(self, messages: List[Message]) -> str:
        # Implementar integra√ß√£o com o novo modelo
        pass
```

3. **Registre no factory:**

```python
# agent_factory.py
class AgentFactory:
    _registry = {
        AgentModelEnum.GEMINI: GeminiAgent,
        AgentModelEnum.NOVO_MODEL: NovoModelAgent,  # Adicione aqui
    }
```

## üõ°Ô∏è Seguran√ßa

- **CORS**: Configurado para aceitar apenas origens autorizadas
- **Valida√ß√£o**: Todas as entradas s√£o validadas com Pydantic
- **API Keys**: Gerenciadas pelo frontend e n√£o armazenadas no backend

## üê≥ Docker

O backend est√° containerizado e pronto para deploy:

```dockerfile
FROM python:3.14-slim
WORKDIR /app
COPY . .
RUN pip install uv && uv sync
CMD ["uv", "run", "uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

## üìù Logs

O sistema utiliza logs estruturados para monitoramento:

- Requisi√ß√µes HTTP
- Erros de integra√ß√£o com APIs
- Valida√ß√µes de dados
- Performance de requests

## ‚ö° Performance

- **Async/Await**: Suporte completo a opera√ß√µes ass√≠ncronas
- **Streaming**: Preparado para respostas em stream (futuro)
- **Connection Pooling**: Reutiliza√ß√£o de conex√µes HTTP
- **Valida√ß√£o Eficiente**: Pydantic para serializa√ß√£o r√°pida

## üîÑ Pr√≥ximas Funcionalidades

- [ ] Implementa√ß√£o de testes automatizados
- [ ] Suporte a streaming de respostas
- [ ] Integra√ß√£o com OpenAI GPT
- [ ] Sistema de rate limiting
- [ ] M√©tricas e observabilidade
- [ ] Cache de respostas
